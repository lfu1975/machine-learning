---
title: "machine learning project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Machine Learning Project


### Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

The goal of the project is to use the available data to predict the manner in which they did the exercise. The report demonstrates the process of predction modeling, cross validation, and accuracy. Finally, the prediction model is applied to predict 20 different test cases.


### Data loading and cleaning
Load the downloaded datasets and conduct data cleaning by removing columns that contain majority of NA missing values or empty values and columns that are unrelated to accelerometer measurements. 
```{r,echo=FALSE}
library(caret)
library(randomForest)
library(corrplot)
training <- read.csv ("/Users/sherryfl/Documents/Coursera/machine learning/pml-training.csv",header=TRUE)
testing <- read.csv ("/Users/sherryfl/Documents/Coursera/machine learning/pml-testing.csv", header=TRUE)

## remove columns containing majority of NAs or empty values
trainset <-  training[,colSums(is.na(training)|training=="") < 0.9 * nrow(training)]
trainset <- trainset[,-c(1:7)] # remove unrelated columns to accelometer reading

testset <-  testing[,colSums(is.na(testing)|testing=="") < 0.9 * nrow(testing)]
testset<- testset[,-c(1:7)]
```


dimension of the clean train dataset
```{r}
dim(trainset)
```

dimension of test dataset
```{r}
dim(testset)

```
The clean training data contains 19622 observations and 53 variables, while the testing dataset contains 20 observations and 53 variables.

### Split training data into a training dataset (70%) and a validation set (30%)
Split the clean training dataset into two parts: real training and validation sets. The validation set will be examined for cross-validation in future steps.
```{r}
set.seed(12345) # For reproducibile purpose
inTrain <- createDataPartition(trainset$classe, p=0.70, list=F)
traindf <- trainset[inTrain, ]
inValid <- trainset[-inTrain, ]
```

### Predict Modeling 

#### Random Forest Modeling
Random Forest algorithm is used because it automatically selects important variables and is robust to correlate covariates and outliers in general. 
```{r, echo=FALSE}
controlRf <- trainControl(method="cv", 5)
modFit_rf <- train(classe~., method="rf", trControl=controlRf,data=traindf)
modFit_rf
```

####Cross Validation 

```{r}
pred_rf <- predict(modFit_rf, newdata=inValid)
cm_rf <- confusionMatrix(pred_rf,inValid$classe)
cm_rf$overall["Accuracy"]
out_of_sample_error <- 1-as.numeric (cm_rf$overall[1])
out_of_sample_error
```

With random forest, we reach an accuracy of  98.93% and the estimated out-of-sample error is (1- 0.9893) *100 = 1.07% by using cross-validation with 5 steps. 


#### prediction of 20 testing cases
```{r}
pred_test <- predict(modFit_rf, newdata=testset)
pred_test
```

## Appendix: Figures
1. Correlation Matrix Visualization (examination of multilineration)
```{r}
png("plot 1.png")
cor_mat <- cor(trainset[,-53])
corrplot(cor_mat,method="color",addgrid.col = "darkgray",title="Correlation Plot",tl.cex=0.4,cl.cex = 1.5)
dev.off()
```


2. Accuracy Visulization of the random forest model
```{r}
png("plot 2.png")
plot(modFit_rf,main="Accuracy of Random forest model by number of predictors")
#plot.getTree(rforest,k=1, labelVar=FALSE)
dev.off()
```

